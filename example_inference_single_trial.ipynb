{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa2ae0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5016bfc",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f278711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "gpu = 0\n",
    "device = f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_nf4 = transformers.AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                 device_map={\"\": device})\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model= model_nf4, #model_id,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbe403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions_answers(text):\n",
    "    messages = [\n",
    "    #    {\"role\": \"system\", \"content\": \"You are an expert in creating key questions from a medical text and extract the answers from the text. Extract 3-10 Q/A pairs without repititions of key entities in the Q/As. Avoid general questions like 'What is the exclusion criteria?'. Make sure an answer is NO MORE than 5 tokens/words. Output as json format like this: {'Question': 'question1', 'Answer': 'answer1', 'Question': 'question2' , 'Answer': 'answer2', ...} \\n Input: \"},\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in creating key questions from a medical text and extract the answers from the text. Extract 3-10 Q/A pairs without repititions of key entities in the Q/As. Avoid general questions like 'What is the exclusion criteria?'. Make sure an answer is NO MORE than 5 tokens/words. Output ONLY json formated Q/A pairs like this: {'Question': 'question1', 'Answer': 'answer1'} \\n {'Question': 'question2' , 'Answer': 'answer2'} \\n ... \\n Input: \"},\n",
    "        {\"role\": \"user\", \"content\": text}]\n",
    "    \n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    #print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "564a8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_q_a_criteria(q_a_criteria):\n",
    "    \"\"\"\n",
    "    Converts a string of question-answer pairs into a list of formatted strings.\n",
    "\n",
    "    Parameters:\n",
    "    q_a_criteria (str): Input string containing question-answer pairs.\n",
    "\n",
    "    Returns:\n",
    "    list: List of strings combining questions and answers.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # Split by newline to handle individual JSON-like entries\n",
    "    for entry in q_a_criteria.split('\\n'):\n",
    "        try:\n",
    "            # Safely evaluate the string to a dictionary\n",
    "            qa_dict = ast.literal_eval(entry)\n",
    "            if 'Question' in qa_dict and 'Answer' in qa_dict:\n",
    "                # Format the question-answer pair\n",
    "                result.append(f\"{qa_dict['Question']} {qa_dict['Answer']}\")\n",
    "        except (ValueError, SyntaxError):\n",
    "            continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887638f",
   "metadata": {},
   "source": [
    "Q/A generation using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78927264",
   "metadata": {},
   "outputs": [],
   "source": [
    "nct_id = 'NCT03134911'\n",
    "intervention = 'DOAC or VKA, VKA'\n",
    "disease = 'Atrial Fibrillation'\n",
    "title = \"Health-related Quality of Life in Patients on Anticoagulants (RE-QUOL)\"\n",
    "outcome_measures = \"Health Related Quality of Life (QoL) (HRQoL) Scores in the Spanish Adaptation of the Sawicki Questionnaire\"\n",
    "keywords = \"Arrhythmias, Cardiac, Heart Diseases, Cardiovascular Diseases, Pathologic Processes, Pathological Conditions, Signs and Symptoms, Atrial Fibrillation, N(4)-oleylcytosine arabinoside\"\n",
    "criteria = 'Inclusion Criteria:\\n The patient is willing and provides written informed consent to participate in this study.\\nThe patient is at least 18 years of age \\nThe patient has a diagnosis of non-valvular atrial fibrillation \\nThe patient is on the same anticoagulant therapy (VKA or DOAC) during at least 6 months and maximum 2 years. \\nIf treated with VKA, availability of % Time in Therapeutic Range (TTR) in past analytical records or enough amount of International Normalized Ratio (INR) measures to calculate it. \\nExclusion Criteria:\\nCurrent participation in any clinical trial of a drug or device\\nContraindication to the use of DOAC or VKA as described in the Summary of Product Characteristics (SmPC).'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd101279",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a_set = parse_q_a_criteria(questions_answers(criteria))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed68f2",
   "metadata": {},
   "source": [
    "Predefined Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b87c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a_set.append('What are the drugs used? '+ intervention)\n",
    "q_a_set.append('What is the disease treated in this trial? '+ disease)\n",
    "q_a_set.append('What is the title of the trial? '+ title)\n",
    "q_a_set.append('What are the outcome measures? '+ outcome_measures)\n",
    "q_a_set.append('What are the keywords? '+ keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4c17b",
   "metadata": {},
   "source": [
    "Load SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fd811fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1174691/2145506959.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('models/global_model.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BioBERT model and tokenizer\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "state_dict = torch.load('models/global_model.pth')\n",
    "\n",
    "# Remove `module.` prefix if present\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith('module.') else k  # remove 'module.' prefix\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d01d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    # Pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the embeddings (hidden states from the last layer)\n",
    "    # outputs.last_hidden_state -> (batch_size, sequence_length, hidden_size)\n",
    "    embeddings = outputs.last_hidden_state.to(device)\n",
    "\n",
    "    # Pool the embeddings (e.g., by taking the mean across the sequence length)\n",
    "    pooled_embeddings = embeddings.mean(dim=1)\n",
    "\n",
    "    return pooled_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c8442",
   "metadata": {},
   "source": [
    "Get embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22bb1ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "#convert q_a_set to string and then embed\n",
    "q_a_string = \" \".join(q_a_set)\n",
    "q_a_embedding = embed_text(q_a_string)\n",
    "print(q_a_embedding.shape)  # Should print: torch.Size([1, hidden_size])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
